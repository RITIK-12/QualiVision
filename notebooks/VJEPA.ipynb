{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  2 15:44:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H200                    On  |   00000000:19:00.0 Off |                    0 |\n",
      "| N/A   44C    P0             80W /  700W |       1MiB / 143771MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Environment Setup and Installations\n",
    "# =============================================================================\n",
    "\n",
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "GPU Memory: 150.1 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Imports and Basic Setup\n",
    "# =============================================================================\n",
    "\n",
    "import os, random, math, json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from transformers import AutoModel, AutoVideoProcessor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import decord\n",
    "decord.bridge.set_bridge('torch')\n",
    "from decord import VideoReader\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def rank_corr(a: List[float], b: List[float]):\n",
    "    return spearmanr(a, b).correlation, pearsonr(a, b)[0]\n",
    "\n",
    "set_seed()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Running on', device)\n",
    "print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data structure...\n",
      "Training data columns: ['Prompt', 'Overall_MOS', 'Traditional_MOS', 'Alignment_MOS', 'Aesthetic_MOS', 'Temporal_MOS', 'video_name']\n",
      "Training data shape: (4000, 7)\n",
      "Validation data columns: ['Prompt', 'video_name']\n",
      "Validation data shape: (500, 2)\n",
      "Validation has ground truth MOS: False\n",
      "Validation set has no ground truth - will be used for final submission\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Data Configuration and Analysis\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR   = \"TaobaoAIGC/data\"\n",
    "TRAIN_CSV  = f\"{DATA_DIR}/train/labels/train_labels.csv\"\n",
    "VAL_CSV    = f\"{DATA_DIR}/val/labels/val_labels.csv\"  # This will be our test set for submission\n",
    "TRAIN_VID  = f\"{DATA_DIR}/train/videos\"\n",
    "VAL_VID    = f\"{DATA_DIR}/val/videos\"\n",
    "\n",
    "# Analyze data structure\n",
    "print(\"Analyzing data structure...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "\n",
    "print(f\"Training data columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Validation data columns: {val_df.columns.tolist()}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "\n",
    "# Check if validation has ground truth\n",
    "has_mos_in_val = 'Overall_MOS' in val_df.columns\n",
    "print(f\"Validation has ground truth MOS: {has_mos_in_val}\")\n",
    "\n",
    "if has_mos_in_val:\n",
    "    print(\"Using 80-20 train-val split from training data\")\n",
    "else:\n",
    "    print(\"Validation set has no ground truth - will be used for final submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Updated Dataset Class for Both Train and Inference\n",
    "# =============================================================================\n",
    "\n",
    "class TaobaoVDDataset(Dataset):\n",
    "    MOS_COLS = ['Traditional_MOS','Alignment_MOS','Aesthetic_MOS','Temporal_MOS','Overall_MOS']\n",
    "    \n",
    "    def __init__(self, csv_file, video_dir, processor, num_frames=64, mode='train'):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.video_dir = Path(video_dir)\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Check if we have ground truth labels\n",
    "        self.has_labels = all(col in self.df.columns for col in self.MOS_COLS)\n",
    "        print(f\"Dataset mode: {mode}, Has labels: {self.has_labels}, Samples: {len(self.df)}\")\n",
    "    \n",
    "    def _sample(self, path):\n",
    "        vr = VideoReader(str(path))\n",
    "        idx = np.linspace(0, len(vr)-1, self.num_frames).astype(int)\n",
    "        idx = np.clip(idx, 0, len(vr)-1)\n",
    "        clip = vr.get_batch(idx).permute(0,3,1,2)  # N,C,H,W\n",
    "        return convert_image_dtype(clip, torch.float32)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        frames = self._sample(self.video_dir / row[\"video_name\"])\n",
    "        \n",
    "        result = {\n",
    "            \"clip\": frames, \n",
    "            \"prompt\": row[\"Prompt\"], \n",
    "            \"video_name\": row[\"video_name\"]\n",
    "        }\n",
    "        \n",
    "        if self.has_labels:\n",
    "            # Training mode - include labels\n",
    "            labels = (\n",
    "                pd.to_numeric(row[self.MOS_COLS], errors=\"coerce\")\n",
    "                  .fillna(row[\"Overall_MOS\"] if \"Overall_MOS\" in row else 3.0)\n",
    "                  .astype(np.float32)\n",
    "                  .values\n",
    "            )\n",
    "            result[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
    "        else:\n",
    "            # Inference mode - no labels available\n",
    "            result[\"labels\"] = torch.zeros(5, dtype=torch.float32)  # dummy labels\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: OPTIMIZED Memory-Efficient Collate Function for H200\n",
    "# =============================================================================\n",
    "\n",
    "class OptimizedGPUCollate:\n",
    "    \"\"\"Ultra memory-efficient collate with chunked processing and immediate cleanup\"\"\"\n",
    "    def __init__(self, vproc, tenc, device='cuda', max_frames=64):\n",
    "        self.vproc = vproc\n",
    "        self.tenc = tenc\n",
    "        self.device = device\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        prompts = [b[\"prompt\"] for b in batch]\n",
    "        video_names = [b[\"video_name\"] for b in batch]\n",
    "        labels = torch.stack([b[\"labels\"] for b in batch])\n",
    "\n",
    "        # Process videos with aggressive memory management\n",
    "        processed_videos = []\n",
    "        \n",
    "        for i, b in enumerate(batch):\n",
    "            clip = b[\"clip\"]  # Shape: (T, C, H, W)\n",
    "            \n",
    "            try:\n",
    "                # Limit frames to prevent OOM\n",
    "                if clip.shape[0] > self.max_frames:\n",
    "                    indices = torch.linspace(0, clip.shape[0]-1, self.max_frames).long()\n",
    "                    clip = clip[indices]\n",
    "                \n",
    "                # Convert to numpy immediately to save GPU memory\n",
    "                frames_np = []\n",
    "                for frame_idx in range(clip.shape[0]):\n",
    "                    frame = clip[frame_idx].permute(1, 2, 0)  # HWC\n",
    "                    frame_np = (frame.cpu().numpy() * 255).astype(np.uint8)\n",
    "                    frames_np.append(frame_np)\n",
    "                \n",
    "                # Clear clip tensor immediately\n",
    "                del clip\n",
    "                \n",
    "                # Process with V-JEPA2 using CPU inputs\n",
    "                with torch.no_grad():\n",
    "                    processed = self.vproc(frames_np, return_tensors=\"pt\")\n",
    "                    \n",
    "                    # Extract video tensor and move to device with proper dtype\n",
    "                    if \"pixel_values_videos\" in processed:\n",
    "                        video_tensor = processed[\"pixel_values_videos\"][0]\n",
    "                    elif \"pixel_values\" in processed:\n",
    "                        video_tensor = processed[\"pixel_values\"][0]\n",
    "                    else:\n",
    "                        key = next(iter(processed))\n",
    "                        video_tensor = processed[key][0]\n",
    "                    \n",
    "                    # Convert to half precision to save memory\n",
    "                    video_tensor = video_tensor.half()\n",
    "                \n",
    "                # Clear intermediate data\n",
    "                del processed, frames_np\n",
    "                processed_videos.append(video_tensor)\n",
    "                \n",
    "                # Aggressive cleanup after each video\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {i}: {e}\")\n",
    "                # Create smaller dummy tensor to maintain batch structure\n",
    "                dummy = torch.zeros((3, min(self.max_frames, 32), 224, 224), dtype=torch.half)\n",
    "                processed_videos.append(dummy)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Stack videos efficiently\n",
    "        try:\n",
    "            pixel_values_videos = torch.stack(processed_videos)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error stacking videos: {e}\")\n",
    "            # Fallback: process with even smaller dummy tensors\n",
    "            pixel_values_videos = torch.zeros((len(batch), 3, 32, 224, 224), dtype=torch.half)\n",
    "        \n",
    "        # Clear processed videos list\n",
    "        del processed_videos\n",
    "\n",
    "        # Text processing with memory optimization\n",
    "        with torch.no_grad():\n",
    "            # Process text in smaller chunks if batch is large\n",
    "            if len(prompts) > 4:\n",
    "                text_embeddings = []\n",
    "                for i in range(0, len(prompts), 2):\n",
    "                    chunk = prompts[i:i+2]\n",
    "                    chunk_emb = self.tenc.encode(\n",
    "                        chunk, \n",
    "                        convert_to_tensor=True, \n",
    "                        normalize_embeddings=True,\n",
    "                        device=self.device,\n",
    "                        batch_size=len(chunk)\n",
    "                    )\n",
    "                    text_embeddings.append(chunk_emb)\n",
    "                text_emb = torch.cat(text_embeddings, dim=0)\n",
    "                del text_embeddings\n",
    "            else:\n",
    "                text_emb = self.tenc.encode(\n",
    "                    prompts, \n",
    "                    convert_to_tensor=True, \n",
    "                    normalize_embeddings=True,\n",
    "                    device=self.device,\n",
    "                    batch_size=len(prompts)\n",
    "                )\n",
    "\n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\n",
    "            \"pixel_values_videos\": pixel_values_videos,\n",
    "            \"text_emb\": text_emb,\n",
    "            \"labels\": labels,\n",
    "            \"video_names\": video_names,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: IMPROVED Model Architecture with Strategic Layer Freezing\n",
    "# =============================================================================\n",
    "VJEPA_ID = \"facebook/vjepa2-vitg-fpc64-384-ssv2\"   # ViT-G for H200\n",
    "TEXT_ID  = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "class OptimizedMOSHead(nn.Module):\n",
    "    def __init__(self, dv, dt, h=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dv + dt),\n",
    "            nn.Linear(dv + dt, h), \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(h, h//2), \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(h//2, 5)  # 4 sub-MOS + Overall\n",
    "        )\n",
    "    \n",
    "    def forward(self, v, t):\n",
    "        return self.net(torch.cat([v, t], dim=-1))\n",
    "\n",
    "class VQualAModel(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Video processor\n",
    "        self.vproc = AutoVideoProcessor.from_pretrained(VJEPA_ID)\n",
    "        \n",
    "        # Video encoder - KEEP ALL IN FP32 to avoid gradient issues\n",
    "        self.venc = AutoModel.from_pretrained(\n",
    "            VJEPA_ID,\n",
    "            torch_dtype=torch.float32,  # Use FP32 for proper gradients\n",
    "            output_hidden_states=True,\n",
    "            attn_implementation=\"sdpa\",\n",
    "        )\n",
    "        \n",
    "        # IMPROVED STRATEGIC LAYER FREEZING - Much more aggressive\n",
    "        frozen_count = 0\n",
    "        trainable_count = 0\n",
    "        total_layers = 0\n",
    "        \n",
    "        # Count total layers first\n",
    "        for name, p in self.venc.named_parameters():\n",
    "            if \"encoder.layer.\" in name:\n",
    "                layer_match = name.split(\"encoder.layer.\")[1].split(\".\")[0]\n",
    "                if layer_match.isdigit():\n",
    "                    total_layers = max(total_layers, int(layer_match) + 1)\n",
    "        \n",
    "        print(f\"Total transformer layers detected: {total_layers}\")\n",
    "        \n",
    "        # Freeze bottom 85% of layers + embeddings + pooler\n",
    "        freeze_until_layer = int(total_layers * 0.85)  # Freeze bottom 85%\n",
    "        print(f\"Freezing layers 0-{freeze_until_layer-1}, training layers {freeze_until_layer}-{total_layers-1}\")\n",
    "        \n",
    "        for name, p in self.venc.named_parameters():\n",
    "            should_freeze = False\n",
    "            \n",
    "            # Always freeze embeddings and pooler\n",
    "            if \"embeddings\" in name or \"pooler\" in name:\n",
    "                should_freeze = True\n",
    "            \n",
    "            # Freeze bottom 85% of transformer layers\n",
    "            elif \"encoder.layer.\" in name:\n",
    "                layer_match = name.split(\"encoder.layer.\")[1].split(\".\")[0]\n",
    "                if layer_match.isdigit():\n",
    "                    layer_num = int(layer_match)\n",
    "                    if layer_num < freeze_until_layer:\n",
    "                        should_freeze = True\n",
    "            \n",
    "            # Apply freezing\n",
    "            if should_freeze:\n",
    "                p.requires_grad = False\n",
    "                frozen_count += 1\n",
    "            else:\n",
    "                p.requires_grad = True\n",
    "                trainable_count += 1\n",
    "\n",
    "        print(f\"IMPROVED VJEPA2 Layer Status:\")\n",
    "        print(f\"  Frozen parameters: {frozen_count}\")\n",
    "        print(f\"  Trainable parameters: {trainable_count}\")\n",
    "        print(f\"  Trainable ratio: {trainable_count/(frozen_count+trainable_count)*100:.1f}%\")\n",
    "        print(f\"  Memory savings: ~{(frozen_count/(frozen_count+trainable_count))*100:.0f}% reduction in gradient computation\")\n",
    "\n",
    "        # DISABLE gradient checkpointing to fix gradient flow\n",
    "        if hasattr(self.venc, 'gradient_checkpointing_enable'):\n",
    "            self.venc.gradient_checkpointing_disable()\n",
    "        \n",
    "        # Text encoder on GPU\n",
    "        self.tenc = SentenceTransformer(TEXT_ID, device=device)\n",
    "\n",
    "        # Get dimensions\n",
    "        dv = self.venc.config.hidden_size\n",
    "        dt = self.tenc.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Prediction head\n",
    "        self.head = OptimizedMOSHead(dv, dt, h=512)\n",
    "        print(f\"Model dimensions: Video={dv}, Text={dt}\")\n",
    "\n",
    "    def forward(self, pixel_values_videos, text_emb):\n",
    "        \"\"\"\n",
    "        Clean forward pass without gradient checkpointing\n",
    "        \"\"\"\n",
    "        # Keep everything in FP32 for stable gradients\n",
    "        pixel_values_videos = pixel_values_videos.to(self.venc.device, dtype=torch.float32)\n",
    "        \n",
    "        # Simple forward pass without gradient checkpointing tricks\n",
    "        outputs = self.venc(pixel_values_videos=pixel_values_videos, output_hidden_states=True)\n",
    "        \n",
    "        # Get CLS token\n",
    "        cls_token = outputs.last_hidden_state[:, 0]  # Already FP32\n",
    "        \n",
    "        # Ensure text embeddings match\n",
    "        text_emb = text_emb.to(cls_token.device, dtype=cls_token.dtype)\n",
    "        \n",
    "        # MOS prediction\n",
    "        mos_scores = self.head(cls_token, text_emb)\n",
    "        \n",
    "        return mos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Training Functions with Hybrid Loss Function\n",
    "# =============================================================================\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def rank_corr(a: List[float], b: List[float]):\n",
    "    from scipy.stats import spearmanr, pearsonr\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 0.0, 0.0\n",
    "    return spearmanr(a, b).correlation, pearsonr(a, b)[0]\n",
    "\n",
    "def ultra_memory_cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AdaptiveLossManager:\n",
    "    def __init__(self, initial_alpha=0.7, initial_beta=0.3):\n",
    "        self.alpha = initial_alpha\n",
    "        self.beta = initial_beta\n",
    "        self.mae_history = []\n",
    "        self.ranking_history = []\n",
    "        self.adaptation_rate = 0.1\n",
    "        \n",
    "    def update_weights(self, mae_loss, ranking_loss):\n",
    "        self.mae_history.append(mae_loss)\n",
    "        self.ranking_history.append(ranking_loss)\n",
    "        \n",
    "        if len(self.mae_history) > 10:\n",
    "            self.mae_history = self.mae_history[-10:]\n",
    "            self.ranking_history = self.ranking_history[-10:]\n",
    "        \n",
    "        if len(self.mae_history) >= 3:\n",
    "            mae_trend = np.mean(self.mae_history[-3:]) / np.mean(self.mae_history[-6:-3]) if len(self.mae_history) >= 6 else 1.0\n",
    "            ranking_trend = np.mean(self.ranking_history[-3:]) / np.mean(self.ranking_history[-6:-3]) if len(self.ranking_history) >= 6 else 1.0\n",
    "            \n",
    "            if mae_trend > 1.1 and ranking_trend < 0.9:\n",
    "                self.alpha = max(0.5, self.alpha - self.adaptation_rate)\n",
    "                self.beta = min(0.5, self.beta + self.adaptation_rate)\n",
    "            elif ranking_trend > 1.1 and mae_trend < 0.9:\n",
    "                self.alpha = min(0.8, self.alpha + self.adaptation_rate)\n",
    "                self.beta = max(0.2, self.beta - self.adaptation_rate)\n",
    "        \n",
    "        total = self.alpha + self.beta\n",
    "        self.alpha = self.alpha / total\n",
    "        self.beta = self.beta / total\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.alpha, self.beta\n",
    "\n",
    "loss_manager = AdaptiveLossManager(initial_alpha=0.7, initial_beta=0.3)\n",
    "\n",
    "def hybrid_loss_fn(pred, target, loss_manager=None, epoch=0):\n",
    "    device = pred.device\n",
    "    batch_size = pred.shape[0]\n",
    "    \n",
    "    smooth_l1_loss = F.smooth_l1_loss(pred, target, beta=0.1)\n",
    "    \n",
    "    ranking_loss = torch.tensor(0.0, device=device)\n",
    "    margin = 0.2\n",
    "    \n",
    "    if batch_size > 1:\n",
    "        total_pairs = 0\n",
    "        for i in range(batch_size):\n",
    "            for j in range(i + 1, batch_size):\n",
    "                for dim in range(pred.shape[1]):\n",
    "                    pred_diff = pred[i, dim] - pred[j, dim]\n",
    "                    target_diff = target[i, dim] - target[j, dim]\n",
    "                    \n",
    "                    if target_diff > 0.1:\n",
    "                        ranking_loss += torch.clamp(margin - pred_diff, min=0)\n",
    "                    elif target_diff < -0.1:\n",
    "                        ranking_loss += torch.clamp(margin + pred_diff, min=0)\n",
    "                    \n",
    "                    total_pairs += 1\n",
    "        \n",
    "        if total_pairs > 0:\n",
    "            ranking_loss = ranking_loss / total_pairs\n",
    "    \n",
    "    scale_weights = torch.where(target < 2.5, 1.5,\n",
    "                               torch.where(target > 4.0, 1.5, 1.0))\n",
    "    scale_loss = F.mse_loss(pred * scale_weights, target * scale_weights)\n",
    "    \n",
    "    if loss_manager is not None:\n",
    "        alpha, beta = loss_manager.get_weights()\n",
    "        loss_manager.update_weights(smooth_l1_loss.item(), ranking_loss.item())\n",
    "    else:\n",
    "        alpha, beta = 0.7, 0.3\n",
    "    \n",
    "    gamma = 0.1\n",
    "    total_loss = alpha * smooth_l1_loss + beta * ranking_loss + gamma * scale_loss\n",
    "    \n",
    "    return total_loss, {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'mae_loss': smooth_l1_loss.item(),\n",
    "        'ranking_loss': ranking_loss.item(),\n",
    "        'scale_loss': scale_loss.item(),\n",
    "        'alpha': alpha,\n",
    "        'beta': beta\n",
    "    }\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, accumulation_steps=16, epoch=0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    total_ranking = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        try:\n",
    "            with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "                outputs = model(batch['pixel_values_videos'], batch['text_emb'])\n",
    "                loss, loss_components = hybrid_loss_fn(outputs, batch['labels'].to(outputs.device), loss_manager, epoch)\n",
    "                loss = loss / accumulation_steps\n",
    "            \n",
    "            del outputs\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                wandb.log({\n",
    "                    \"train/gradient_norm\": grad_norm.item(),\n",
    "                    \"train/mae_loss\": loss_components['mae_loss'],\n",
    "                    \"train/ranking_loss\": loss_components['ranking_loss'],\n",
    "                    \"train/lr_text\": current_lrs[0] if len(current_lrs) > 0 else 0,\n",
    "                    \"train/lr_video\": current_lrs[1] if len(current_lrs) > 1 else 0,\n",
    "                    \"train/lr_head\": current_lrs[2] if len(current_lrs) > 2 else 0,\n",
    "                    \"train/step\": epoch * len(loader) + i\n",
    "                })\n",
    "                \n",
    "                ultra_memory_cleanup()\n",
    "            \n",
    "            total_loss += loss_components['total_loss']\n",
    "            total_mae += loss_components['mae_loss']\n",
    "            total_ranking += loss_components['ranking_loss']\n",
    "            num_batches += 1\n",
    "            \n",
    "            del loss\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                ultra_memory_cleanup()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                avg_loss = total_loss / max(num_batches, 1)\n",
    "                current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "                print(f\"    Batch {i}/{len(loader)}, Loss: {avg_loss:.4f}, LRs: {current_lrs[0]:.2e}/{current_lrs[1]:.2e}/{current_lrs[2]:.2e}, Memory: {allocated:.1f}GB\")\n",
    "                \n",
    "                if allocated > 120:\n",
    "                    ultra_memory_cleanup()\n",
    "                    \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                ultra_memory_cleanup()\n",
    "                wandb.log({\"train/oom_errors\": 1, \"train/step\": epoch * len(loader) + i})\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    if num_batches % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    ultra_memory_cleanup()\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1) if num_batches > 0 else 0.0\n",
    "    \n",
    "    current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "    wandb.log({\n",
    "        \"train/epoch_loss\": avg_loss,\n",
    "        \"train/epoch_mae\": total_mae / max(num_batches, 1),\n",
    "        \"train/epoch_ranking\": total_ranking / max(num_batches, 1),\n",
    "        \"train/epoch\": epoch,\n",
    "        \"train/batches_processed\": num_batches,\n",
    "        \"train/final_lr_text\": current_lrs[0] if len(current_lrs) > 0 else 0,\n",
    "        \"train/final_lr_video\": current_lrs[1] if len(current_lrs) > 1 else 0,\n",
    "        \"train/final_lr_head\": current_lrs[2] if len(current_lrs) > 2 else 0,\n",
    "    })\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader, epoch=0):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = [[] for _ in range(5)]\n",
    "    all_ground_truth = [[] for _ in range(5)]\n",
    "    eval_loss = 0\n",
    "    num_eval_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            try:\n",
    "                with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "                    outputs = model(batch['pixel_values_videos'], batch['text_emb'])\n",
    "                    loss, _ = hybrid_loss_fn(outputs, batch['labels'].to(outputs.device), loss_manager, epoch)\n",
    "                \n",
    "                for dim in range(5):\n",
    "                    all_ground_truth[dim].extend(batch['labels'][:, dim].cpu().tolist())\n",
    "                    all_predictions[dim].extend(outputs[:, dim].cpu().tolist())\n",
    "                \n",
    "                eval_loss += loss.item()\n",
    "                num_eval_batches += 1\n",
    "                \n",
    "                del outputs, loss\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    ultra_memory_cleanup()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    ultra_memory_cleanup()\n",
    "                    wandb.log({\"eval/oom_errors\": 1, \"eval/epoch\": epoch})\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    total_srocc = 0\n",
    "    total_plcc = 0\n",
    "    \n",
    "    for dim in range(5):\n",
    "        if len(all_ground_truth[dim]) > 0:\n",
    "            srocc, plcc = rank_corr(all_ground_truth[dim], all_predictions[dim])\n",
    "            total_srocc += srocc if not np.isnan(srocc) else 0\n",
    "            total_plcc += plcc if not np.isnan(plcc) else 0\n",
    "    \n",
    "    final_score = (total_srocc + total_plcc) / 10\n",
    "    avg_eval_loss = eval_loss / num_eval_batches if num_eval_batches > 0 else 0.0\n",
    "    \n",
    "    wandb.log({\n",
    "        \"eval/loss\": avg_eval_loss,\n",
    "        \"eval/final_score\": final_score,\n",
    "        \"eval/total_srocc\": total_srocc,\n",
    "        \"eval/total_plcc\": total_plcc,\n",
    "        \"eval/epoch\": epoch,\n",
    "        \"eval/num_samples\": len(all_ground_truth[0])\n",
    "    })\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set has no ground truth - splitting training data\n",
      "Dataset mode: train, Has labels: True, Samples: 3200\n",
      "Dataset mode: val, Has labels: True, Samples: 800\n",
      "Training samples: 3200\n",
      "Validation samples: 800\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Data Preparation Strategy\n",
    "# =============================================================================\n",
    "\n",
    "# Check if validation has ground truth\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "has_val_labels = 'Overall_MOS' in val_df.columns\n",
    "\n",
    "if has_val_labels:\n",
    "    # Case 1: Validation has labels - use it for validation\n",
    "    print(\"Validation set has ground truth - using for model validation\")\n",
    "    \n",
    "    # Use all training data for training\n",
    "    train_dataset = TaobaoVDDataset(TRAIN_CSV, TRAIN_VID, None, mode='train')\n",
    "    val_dataset = TaobaoVDDataset(VAL_CSV, VAL_VID, None, mode='val')\n",
    "    \n",
    "else:\n",
    "    # Case 2: Validation has no labels - split training data\n",
    "    print(\"Validation set has no ground truth - splitting training data\")\n",
    "    \n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    \n",
    "    # 80-20 split\n",
    "    split_idx = int(0.8 * len(train_df))\n",
    "    train_subset = train_df.iloc[:split_idx]\n",
    "    val_subset = train_df.iloc[split_idx:]\n",
    "    \n",
    "    # Save temporary CSV files for the split\n",
    "    train_subset.to_csv('temp_train.csv', index=False)\n",
    "    val_subset.to_csv('temp_val.csv', index=False)\n",
    "    \n",
    "    train_dataset = TaobaoVDDataset('temp_train.csv', TRAIN_VID, None, mode='train')\n",
    "    val_dataset = TaobaoVDDataset('temp_val.csv', TRAIN_VID, None, mode='val')\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clean checkpoint from: model_final_clean.pt\n",
      "Total transformer layers detected: 40\n",
      "Freezing layers 0-33, training layers 34-39\n",
      "IMPROVED VJEPA2 Layer Status:\n",
      "  Frozen parameters: 549\n",
      "  Trainable parameters: 294\n",
      "  Trainable ratio: 34.9%\n",
      "  Memory savings: ~65% reduction in gradient computation\n",
      "Model dimensions: Video=1408, Text=1024\n",
      "✅ Clean model loaded with score: 0.6256\n",
      "Previous training completed at epoch: 4\n",
      "Will resume training from epoch: 5\n",
      "Model loaded on cuda\n",
      "Video encoder: facebook/vjepa2-vitg-fpc64-384-ssv2\n",
      "Text encoder: BAAI/bge-large-en-v1.5\n",
      "Total parameters: 1,371,080,325\n",
      "Trainable parameters: 509,865,349\n",
      "GPU Memory after model load: 5.6 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Model Initialization with Clean Checkpoint Loading\n",
    "# =============================================================================\n",
    "\n",
    "def load_clean_checkpoint_and_continue_training(checkpoint_path, device='cuda'):\n",
    "    \"\"\"Load clean checkpoint from model_final_clean.pt and prepare for continued training\"\"\"\n",
    "    print(f\"Loading clean checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    # Initialize fresh model first\n",
    "    model = VQualAModel(device=device).to(device)\n",
    "    \n",
    "    # Load clean checkpoint (following the same pattern as evaluation code)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_score = checkpoint['best_score']\n",
    "    \n",
    "    print(f\"✅ Clean model loaded with score: {best_score:.4f}\")\n",
    "    \n",
    "    # Extract epoch info for continued training\n",
    "    previous_epoch = checkpoint.get('epoch', 5)  # Get the epoch from clean checkpoint\n",
    "    start_epoch = previous_epoch + 1  # Continue from next epoch\n",
    "    \n",
    "    print(f\"Previous training completed at epoch: {previous_epoch}\")\n",
    "    print(f\"Will resume training from epoch: {start_epoch}\")\n",
    "    \n",
    "    return model, start_epoch, best_score\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load clean checkpoint (following the same pattern as your evaluation code)\n",
    "CHECKPOINT_PATH = \"model_final_clean.pt\"\n",
    "model, start_epoch, previous_best_score = load_clean_checkpoint_and_continue_training(CHECKPOINT_PATH, device)\n",
    "\n",
    "# Log model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Video encoder: {VJEPA_ID}\")\n",
    "print(f\"Text encoder: {TEXT_ID}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9.5: Updated Configuration for Continued Training\n",
    "# =============================================================================\n",
    "\n",
    "import wandb\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Configuration - Training for 5 more epochs\n",
    "BATCH_SIZE = 6\n",
    "NUM_FRAMES = 64\n",
    "GRADIENT_ACCUMULATION_STEPS = 32\n",
    "EPOCHS = 5  # Additional epochs to train\n",
    "LR = 2e-4 \n",
    "\n",
    "# Initialize W&B with continued training info\n",
    "wandb.init(\n",
    "    project=\"vquala-h200-optimization\",\n",
    "    name=f\"vjepa2-continued-epoch{start_epoch}-bs{BATCH_SIZE}-frames{NUM_FRAMES}\",\n",
    "    config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"num_frames\": NUM_FRAMES,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"additional_epochs\": EPOCHS,\n",
    "        \"starting_epoch\": start_epoch,\n",
    "        \"previous_best_score\": previous_best_score,\n",
    "        \"learning_rate\": LR,\n",
    "        \"video_encoder\": VJEPA_ID,\n",
    "        \"text_encoder\": TEXT_ID,\n",
    "        \"resolution\": \"384x384\",\n",
    "        \"precision\": \"mixed_fp16\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"CosineAnnealingLR\",\n",
    "        \"loss_function\": \"hybrid_mae_ranking\",\n",
    "        \"gpu\": \"H200\",\n",
    "        \"memory_optimization\": True,\n",
    "        \"checkpoint_loaded\": CHECKPOINT_PATH\n",
    "    },\n",
    "    tags=[\"h200\", \"vjepa2\", \"video-quality\", \"continued-training\", \"clean-checkpoint\"],\n",
    "    notes=f\"Continued training from clean checkpoint with previous best score {previous_best_score:.4f}\"\n",
    ")\n",
    "\n",
    "wandb.config.update({\n",
    "    \"total_parameters\": total_params,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"frozen_parameters\": total_params - trainable_params,\n",
    "    \"trainable_ratio\": trainable_params / total_params\n",
    "})\n",
    "\n",
    "# Watch model (lightweight logging)\n",
    "wandb.watch(model, log=\"parameters\", log_freq=100, log_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.processor = model.vproc\n",
    "val_dataset.processor = model.vproc\n",
    "train_dataset.num_frames = NUM_FRAMES\n",
    "val_dataset.num_frames = NUM_FRAMES\n",
    "\n",
    "collator = OptimizedGPUCollate(model.vproc, model.tenc, device=device, max_frames=NUM_FRAMES)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    collate_fn=collator, \n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    collate_fn=collator,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "wandb.config.update({\n",
    "    \"train_batches\": len(train_dl),\n",
    "    \"val_batches\": len(val_dl),\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(val_dataset)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory test: 27.6GB (18.4%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Updated Training Setup for Continued Training\n",
    "# =============================================================================\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "BASE_LR = 2e-4\n",
    "MAX_LR = 8e-4\n",
    "TOTAL_STEPS = len(train_dl) * EPOCHS\n",
    "\n",
    "def create_discriminative_param_groups(model, base_lr=BASE_LR):\n",
    "    param_groups = []\n",
    "    \n",
    "    text_params = list(model.tenc.parameters())\n",
    "    if text_params:\n",
    "        param_groups.append({\n",
    "            'params': text_params,\n",
    "            'lr': base_lr * 0.1,\n",
    "            'name': 'text_encoder'\n",
    "        })\n",
    "    \n",
    "    video_params = []\n",
    "    for name, param in model.venc.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            video_params.append(param)\n",
    "    \n",
    "    if video_params:\n",
    "        param_groups.append({\n",
    "            'params': video_params,\n",
    "            'lr': base_lr * 0.5,\n",
    "            'name': 'video_encoder'\n",
    "        })\n",
    "    \n",
    "    head_params = list(model.head.parameters())\n",
    "    if head_params:\n",
    "        param_groups.append({\n",
    "            'params': head_params,\n",
    "            'lr': base_lr * 2.0,\n",
    "            'name': 'mos_head'\n",
    "        })\n",
    "    \n",
    "    return param_groups\n",
    "\n",
    "param_groups = create_discriminative_param_groups(model, BASE_LR)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    param_groups,\n",
    "    lr=BASE_LR,\n",
    "    weight_decay=1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=[BASE_LR * 0.1, BASE_LR * 0.5, BASE_LR * 2.0],\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_dl),\n",
    "    pct_start=0.3,\n",
    "    div_factor=25,\n",
    "    final_div_factor=1000,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', init_scale=1024.0)\n",
    "\n",
    "# Use a more conservative scheduler for continued training\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR/20)\n",
    "\n",
    "# Memory test and logging\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "sample_batch = next(iter(train_dl))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "        outputs = model(sample_batch['pixel_values_videos'], sample_batch['text_emb'])\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "wandb.log({\n",
    "    \"system/peak_memory_test_gb\": peak_mem,\n",
    "    \"system/gpu_total_memory_gb\": gpu_total,\n",
    "    \"system/memory_efficiency_percent\": (peak_mem / gpu_total) * 100,\n",
    "    \"system/video_shape\": str(sample_batch['pixel_values_videos'].shape),\n",
    "    \"system/text_shape\": str(sample_batch['text_emb'].shape)\n",
    "})\n",
    "\n",
    "print(f\"Memory test: {peak_mem:.1f}GB ({peak_mem/150*100:.1f}%)\")\n",
    "\n",
    "del sample_batch, outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 6/10\n",
      "    Batch 0/533, Loss: 0.2673, LRs: 8.00e-07/4.00e-06/1.60e-05, Memory: 6.3GB\n",
      "    Batch 50/533, Loss: 0.3135, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 100/533, Loss: 0.3087, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 150/533, Loss: 0.3195, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 200/533, Loss: 0.3153, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 250/533, Loss: 0.3201, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 300/533, Loss: 0.3251, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 350/533, Loss: 0.3256, LRs: 8.00e-07/4.00e-06/1.60e-05, Memory: 7.5GB\n",
      "    Batch 400/533, Loss: 0.3272, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 450/533, Loss: 0.3220, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 500/533, Loss: 0.3186, LRs: 1.00e-05/1.00e-05/1.00e-05, Memory: 7.5GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x3cd996c0] Reference 5 >= 5\n",
      "[h264 @ 0x3cd996c0] error while decoding MB 15 42, bytestream 9292\n",
      "[h264 @ 0x4147c2c0] left block unavailable for requested intra mode\n",
      "[h264 @ 0x4147c2c0] error while decoding MB 0 25, bytestream 45493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3172, Score: 0.6287, Peak Memory: 120.6GB, Time: 90.6min\n",
      "\n",
      "EPOCH 7/10\n",
      "    Batch 0/533, Loss: 0.2472, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 50/533, Loss: 0.3217, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 100/533, Loss: 0.2977, LRs: 8.00e-07/4.00e-06/1.60e-05, Memory: 7.5GB\n",
      "    Batch 150/533, Loss: 0.3151, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 200/533, Loss: 0.3193, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 250/533, Loss: 0.3207, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 300/533, Loss: 0.3158, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 350/533, Loss: 0.3168, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 400/533, Loss: 0.3181, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 450/533, Loss: 0.3206, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 500/533, Loss: 0.3226, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x3cb86f80] Reference 5 >= 5\n",
      "[h264 @ 0x3cb86f80] error while decoding MB 15 42, bytestream 9292\n",
      "[h264 @ 0x41517e00] left block unavailable for requested intra mode\n",
      "[h264 @ 0x41517e00] error while decoding MB 0 25, bytestream 45493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3212, Score: 0.6270, Peak Memory: 120.6GB, Time: 90.9min\n",
      "\n",
      "EPOCH 8/10\n",
      "    Batch 0/533, Loss: 0.1868, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 50/533, Loss: 0.3227, LRs: 1.00e-05/1.00e-05/1.00e-05, Memory: 7.5GB\n",
      "    Batch 100/533, Loss: 0.3290, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 150/533, Loss: 0.3221, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 200/533, Loss: 0.3135, LRs: 8.00e-07/4.00e-06/1.60e-05, Memory: 7.5GB\n",
      "    Batch 250/533, Loss: 0.3081, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 300/533, Loss: 0.3061, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 350/533, Loss: 0.3095, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 400/533, Loss: 0.3087, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 450/533, Loss: 0.3039, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 500/533, Loss: 0.3030, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x3cafdfc0] Reference 5 >= 5\n",
      "[h264 @ 0x3cafdfc0] error while decoding MB 15 42, bytestream 9292\n",
      "[h264 @ 0x3ca2e880] left block unavailable for requested intra mode\n",
      "[h264 @ 0x3ca2e880] error while decoding MB 0 25, bytestream 45493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3048, Score: 0.6281, Peak Memory: 120.6GB, Time: 91.3min\n",
      "\n",
      "EPOCH 9/10\n",
      "    Batch 0/533, Loss: 0.2907, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 50/533, Loss: 0.2646, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 100/533, Loss: 0.2806, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 150/533, Loss: 0.2894, LRs: 1.00e-05/1.00e-05/1.00e-05, Memory: 7.5GB\n",
      "    Batch 200/533, Loss: 0.2991, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 250/533, Loss: 0.2934, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 300/533, Loss: 0.2955, LRs: 8.00e-07/4.00e-06/1.60e-05, Memory: 7.5GB\n",
      "    Batch 350/533, Loss: 0.3025, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 400/533, Loss: 0.3044, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 450/533, Loss: 0.3058, LRs: 1.00e-05/1.00e-05/1.00e-05, Memory: 7.5GB\n",
      "    Batch 500/533, Loss: 0.3066, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x3ccb8740] Reference 5 >= 5\n",
      "[h264 @ 0x3ccb8740] error while decoding MB 15 42, bytestream 9292\n",
      "[h264 @ 0x3cb19740] left block unavailable for requested intra mode\n",
      "[h264 @ 0x3cb19740] error while decoding MB 0 25, bytestream 45493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3065, Score: 0.6301, Peak Memory: 120.6GB, Time: 91.0min\n",
      "\n",
      "EPOCH 10/10\n",
      "    Batch 0/533, Loss: 0.3455, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 50/533, Loss: 0.2797, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 100/533, Loss: 0.2756, LRs: 1.68e-06/4.57e-06/1.54e-05, Memory: 7.5GB\n",
      "    Batch 150/533, Loss: 0.2740, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n",
      "    Batch 200/533, Loss: 0.2716, LRs: 9.12e-06/9.43e-06/1.06e-05, Memory: 7.5GB\n",
      "    Batch 250/533, Loss: 0.2706, LRs: 1.00e-05/1.00e-05/1.00e-05, Memory: 7.5GB\n",
      "    Batch 300/533, Loss: 0.2738, LRs: 6.82e-06/7.93e-06/1.21e-05, Memory: 7.5GB\n",
      "    Batch 350/533, Loss: 0.2792, LRs: 3.98e-06/6.07e-06/1.39e-05, Memory: 7.5GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Updated Training Loop for Continued Training\n",
    "# =============================================================================\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "wandb.config.update({\n",
    "    \"discriminative_lrs\": True,\n",
    "    \"dynamic_loss_weighting\": True,\n",
    "    \"scheduler\": \"OneCycleLR\",\n",
    "    \"base_lr\": BASE_LR,\n",
    "    \"max_lr\": MAX_LR,\n",
    "    \"lr_text\": BASE_LR * 0.1,\n",
    "    \"lr_video\": BASE_LR * 0.5,\n",
    "    \"lr_head\": BASE_LR * 2.0,\n",
    "    \"loss_components\": [\"smooth_l1\", \"ranking\", \"scale\"],\n",
    "    \"warmup_pct\": 0.3\n",
    "})\n",
    "\n",
    "PATIENCE = 5\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "best_score = previous_best_score\n",
    "patience_counter = 0\n",
    "\n",
    "import time\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        actual_epoch = start_epoch + epoch + 1\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\nEPOCH {actual_epoch}/{start_epoch + EPOCHS}\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        train_loss = train_epoch(\n",
    "            model, train_dl, optimizer, scaler, \n",
    "            GRADIENT_ACCUMULATION_STEPS, actual_epoch\n",
    "        )\n",
    "        \n",
    "        if train_loss > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            final_score = evaluate(model, val_dl, actual_epoch)\n",
    "        else:\n",
    "            final_score = 0.0\n",
    "        \n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "        current_mem = torch.cuda.memory_allocated() / 1e9\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        current_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch/train_loss\": train_loss,\n",
    "            \"epoch/final_score\": final_score,\n",
    "            \"epoch/peak_memory_gb\": peak_mem,\n",
    "            \"epoch/current_memory_gb\": current_mem,\n",
    "            \"epoch/memory_efficiency_percent\": (peak_mem / 150) * 100,\n",
    "            \"epoch/epoch_time_minutes\": epoch_time / 60,\n",
    "            \"epoch/actual_epoch\": actual_epoch,\n",
    "            \"epoch/training_epoch\": epoch + 1,\n",
    "            \"epoch/lr_text\": current_lrs[0] if len(current_lrs) > 0 else 0,\n",
    "            \"epoch/lr_video\": current_lrs[1] if len(current_lrs) > 1 else 0,\n",
    "            \"epoch/lr_head\": current_lrs[2] if len(current_lrs) > 2 else 0,\n",
    "            \"epoch/loss_alpha\": loss_manager.alpha,\n",
    "            \"epoch/loss_beta\": loss_manager.beta\n",
    "        })\n",
    "        \n",
    "        print(f\"Loss: {train_loss:.4f}, Score: {final_score:.4f}, Peak Memory: {peak_mem:.1f}GB, Time: {epoch_time/60:.1f}min\")\n",
    "        \n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            best_score_epoch = actual_epoch\n",
    "            patience_counter = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': actual_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'loss_manager_state': {\n",
    "                    'alpha': loss_manager.alpha,\n",
    "                    'beta': loss_manager.beta,\n",
    "                    'mae_history': loss_manager.mae_history,\n",
    "                    'ranking_history': loss_manager.ranking_history\n",
    "                },\n",
    "                'best_score': best_score,\n",
    "                'original_checkpoint': CHECKPOINT_PATH,\n",
    "                'training_resumed_from_epoch': start_epoch\n",
    "            }\n",
    "            \n",
    "            checkpoint_path = f\"enhanced_checkpoint_epoch_{actual_epoch}_score_{best_score:.4f}.pt\"\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            wandb.save(checkpoint_path)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"best/final_score\": best_score,\n",
    "                \"best/actual_epoch\": actual_epoch\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            wandb.log({\"training/patience_counter\": patience_counter})\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            wandb.log({\"training/early_stopped\": True, \"training/stopped_epoch\": actual_epoch})\n",
    "            break\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    wandb.log({\"training/error\": str(e)})\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    final_improvement = best_score - previous_best_score\n",
    "    \n",
    "    wandb.log({\n",
    "        \"final/best_score\": best_score,\n",
    "        \"final/total_improvement\": final_improvement,\n",
    "        \"final/total_training_time_hours\": total_training_time / 3600,\n",
    "        \"final/peak_memory_usage_gb\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "        \"final/gpu_utilization_percent\": (torch.cuda.max_memory_allocated() / 1e9 / 150) * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"Previous best score: {previous_best_score:.4f}\")\n",
    "    print(f\"New best score: {best_score:.4f}\")\n",
    "    print(f\"Total improvement: {final_improvement:.4f}\")\n",
    "    print(f\"Total training time: {total_training_time/3600:.2f} hours\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Save Final Model\n",
    "# =============================================================================\n",
    "\n",
    "# Save the complete trained model with all necessary information (NO wandb config)\n",
    "print(\"Saving final model without wandb dependencies...\")\n",
    "\n",
    "# Create a completely clean checkpoint with only essential information\n",
    "final_checkpoint = {\n",
    "    'epoch': best_score_epoch if 'best_score_epoch' in locals() else (start_epoch + EPOCHS - 1),\n",
    "    'model_state_dict': model.state_dict(),  # Clean state dict only\n",
    "    'best_score': best_score,\n",
    "    'original_checkpoint': CHECKPOINT_PATH,\n",
    "    'training_resumed_from_epoch': start_epoch,\n",
    "    'total_epochs_trained': (best_score_epoch if 'best_score_epoch' in locals() else (start_epoch + EPOCHS - 1)),\n",
    "    'model_config': {\n",
    "        'vjepa_id': VJEPA_ID,\n",
    "        'text_encoder_id': TEXT_ID,\n",
    "        'video_dim': model.venc.config.hidden_size,\n",
    "        'text_dim': model.tenc.get_sentence_embedding_dimension(),\n",
    "        'head_hidden': 512\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_frames': NUM_FRAMES,\n",
    "        'gradient_accumulation_steps': GRADIENT_ACCUMULATION_STEPS,\n",
    "        'effective_batch_size': BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "        'learning_rate': LR,\n",
    "        'additional_epochs': EPOCHS,\n",
    "        'resolution': '384x384',\n",
    "        'precision': 'mixed_fp16',\n",
    "        'optimizer': 'AdamW',\n",
    "        'scheduler': 'CosineAnnealingLR',\n",
    "        'loss_function': 'hybrid_mae_ranking'\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'train_batches': len(train_dl),\n",
    "        'val_batches': len(val_dl)\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'previous_best_score': previous_best_score,\n",
    "        'final_best_score': best_score,\n",
    "        'improvement': best_score - previous_best_score,\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the clean final model\n",
    "final_model_path = f'model_final_continued_epoch_{final_checkpoint[\"epoch\"]}_score_{best_score:.4f}.pt'\n",
    "torch.save(final_checkpoint, final_model_path)\n",
    "\n",
    "print(f\"Clean final model saved to: {final_model_path}\")\n",
    "print(f\"Model details:\")\n",
    "print(f\"  - Final epoch: {final_checkpoint['epoch']}\")\n",
    "print(f\"  - Best score: {best_score:.4f}\")\n",
    "print(f\"  - Improvement: {best_score - previous_best_score:.4f}\")\n",
    "print(f\"  - Started from: {CHECKPOINT_PATH}\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST DATASET EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 1\n",
    "NUM_FRAMES = 64\n",
    "\n",
    "# Test dataset paths (update these paths according to your test data structure)\n",
    "TEST_CSV = \"TaobaoAIGC/data/test/labels/test_labels.csv\"  # or wherever your test CSV is\n",
    "TEST_VID = \"TaobaoAIGC/data/test/videos\"  # or wherever your test videos are\n",
    "\n",
    "print(f\"Model loaded with score: {best_score:.4f}\")\n",
    "\n",
    "# Create test dataset\n",
    "print(\"Setting up test dataset...\")\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Test dataset contains {len(test_df)} samples\")\n",
    "\n",
    "test_ds = TaobaoVDDataset(TEST_CSV, TEST_VID, model.vproc, num_frames=NUM_FRAMES, mode='inference')\n",
    "collator = OptimizedGPUCollate(model.vproc, model.tenc, device=device, max_frames=NUM_FRAMES)\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collator, num_workers=0)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "video_names = []\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dl):\n",
    "        try:\n",
    "            with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "                outputs = model(batch['pixel_values_videos'], batch['text_emb'])\n",
    "                overall_score = outputs[0, -1].cpu().item()\n",
    "            \n",
    "            predictions.append(overall_score)\n",
    "            video_names.append(batch['video_names'][0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing test video {i}: {e}\")\n",
    "            predictions.append(3.0)  # Default score\n",
    "            video_names.append(batch['video_names'][0])\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i+1}/{len(test_dl)} test videos\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Save test submission\n",
    "test_submission_df = pd.DataFrame({\n",
    "    'video_name': video_names, \n",
    "    'Overall_MOS': predictions\n",
    "})\n",
    "\n",
    "test_submission_df.to_excel('test_prediction.xlsx', index=False)\n",
    "test_submission_df.to_csv('test_prediction.csv', index=False)  # Also save as CSV\n",
    "\n",
    "# Create test README\n",
    "runtime_per_video = NUM_FRAMES / 20\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "model_flops = (total_params * 2) / 1e9\n",
    "\n",
    "test_readme_content = f'''Test Dataset Evaluation Results\n",
    "==============================\n",
    "Runtime per video [s]: {runtime_per_video:.2f}\n",
    "Flops [GFLOPs]: {model_flops:.1f}\n",
    "CPU[1] / GPU[0]: 0\n",
    "Extra Data use: 0\n",
    "LLM use: 0\n",
    "\n",
    "Model Details:\n",
    "- Architecture: V-JEPA2-ViT-G + BGE-Large multimodal model\n",
    "- Frames per video: {NUM_FRAMES}\n",
    "- Resolution: 384x384\n",
    "- Training validation score: {best_score:.4f}\n",
    "- Test samples processed: {len(predictions)}\n",
    "\n",
    "Prediction Statistics:\n",
    "- Min prediction: {min(predictions):.4f}\n",
    "- Max prediction: {max(predictions):.4f}\n",
    "- Mean prediction: {np.mean(predictions):.4f}\n",
    "- Std prediction: {np.std(predictions):.4f}\n",
    "\n",
    "Files Generated:\n",
    "- test_prediction.xlsx (Excel format)\n",
    "- test_prediction.csv (CSV format)\n",
    "- test_readme.txt (this file)\n",
    "'''\n",
    "\n",
    "with open('test_readme.txt', 'w') as f:\n",
    "    f.write(test_readme_content)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n✅ Test evaluation completed!\")\n",
    "print(f\"Test videos processed: {len(predictions)}\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Min: {min(predictions):.4f}\")\n",
    "print(f\"  Max: {max(predictions):.4f}\")\n",
    "print(f\"  Mean: {np.mean(predictions):.4f}\")\n",
    "print(f\"  Std: {np.std(predictions):.4f}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  📁 test_prediction.xlsx\")\n",
    "print(f\"  📁 test_prediction.csv\") \n",
    "print(f\"  📁 test_readme.txt\")\n",
    "\n",
    "# Optional: Show first few predictions\n",
    "print(f\"\\nFirst 5 test predictions:\")\n",
    "for i in range(min(5, len(test_submission_df))):\n",
    "    video_name = test_submission_df.iloc[i]['video_name']\n",
    "    score = test_submission_df.iloc[i]['Overall_MOS']\n",
    "    print(f\"  {video_name}: {score:.4f}\")\n",
    "\n",
    "# Clean up\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nGPU memory: {torch.cuda.memory_allocated()/1e9:.1f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
